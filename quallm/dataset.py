
from typing import Optional, Union, List, Dict, Any
import pandas as pd
import numpy as np
from quallm.utils.dataframe_utils import is_nan_or_none


class SampleString(str):
    """
    A custom string-like class for storing samples generated by Dataset.from_samples(),
    including sample metadata.
    """
    def __new__(cls, content, sample_size, dataset_size, indices, random_state):
        return super().__new__(cls, content)

    def __init__(self, content, sample_size, dataset_size, indices, random_state):
        super().__init__()
        self._sample_size = sample_size
        self._dataset_size = dataset_size
        self._indices = indices
        self._random_state = random_state

    @property
    def sample_size(self):
        return self._sample_size

    @property
    def dataset_size(self):
        return self._dataset_size

    @property
    def indices(self):
        return self._indices

    @property
    def random_state(self):
        return self._random_state

    def __str__(self):
        return f"""Sample of {self.sample_size} of {self.dataset_size} observations:\n{'='*10}\n{super().__str__()}"""


class ClusterSampleString(SampleString):
    """
    Extended SampleString for cluster-based samples that includes cluster metadata.
    Follows the established pattern from SampleString with additional cluster-specific attributes.
    """
    def __new__(cls, content, sample_size, dataset_size, indices, random_state,
                source_clusters, combination_type, cluster_stats):
        return super().__new__(cls, content, sample_size, dataset_size, indices, random_state)

    def __init__(self, content, sample_size, dataset_size, indices, random_state,
                 source_clusters, combination_type, cluster_stats):
        super().__init__(content, sample_size, dataset_size, indices, random_state)
        self._source_clusters = source_clusters
        self._combination_type = combination_type
        self._cluster_stats = cluster_stats

    @property
    def source_clusters(self):
        """List of cluster IDs that contributed to this sample."""
        return self._source_clusters

    @property 
    def combination_type(self):
        """Type of combination: 'individual', 'pair', 'cluster-random', 'random'."""
        return self._combination_type

    @property
    def cluster_stats(self):
        """Dict with cluster statistics: sizes, outlier counts, etc."""
        return self._cluster_stats

    def __str__(self):
        cluster_desc = ", ".join(map(str, self.source_clusters))
        return f"""Cluster sample ({self.combination_type}): clusters [{cluster_desc}]
Sample of {self.sample_size} of {self.dataset_size} observations:
{'='*10}
{super(SampleString, self).__str__()}"""


class Dataset(List[Dict[str, str]]):
    """
    A custom list-like class for managing data to be inserted into prompts.

    This class extends the built-in list type to store and manage a collection of 
    observations, where each observation is represented as a dictionary. The keys 
    in these dictionaries correspond to data argument names, and the values are the 
    corresponding values (data) to be inserted into a prompt at inference.

    Attributes:
        data_args (List[str]): The expected argument names for each data point.

    Args:
        data: The input data. Can be a single dictionary, a string (for single-argument datasets),
              a list/array/series of dictionaries or values, or a DataFrame.
        data_args (str or List[str]): The name(s) of the data argument(s) expected in each observation.

    Raises:
        ValueError: If the input data structure doesn't match the expected format based on data_args,
                    or if there's a mismatch between the keys in the data and the specified data_args.

    Example:
        >>> dataset = Dataset(["text1", "text2", "text3"], "input_text")
        >>> print(dataset[0])
        {'input_text': 'text1'}
    """
    def __init__(self,
                 data: Union[str, Dict[str, Any], List[Any], pd.Series, pd.DataFrame, np.ndarray, List[Dict[str, Any]]],
                 data_args: Union[str, List[str]]):
        if isinstance(data_args, str):
            data_args = [data_args]
        self.data_args = data_args
        super().__init__(self.standardize_input(data))
        
    def standardize_input(self, data) -> List[Dict[str, str]]:
        """Convert input into data list of dicts with one dict per observation"""
        data_args = self.data_args
        n_expected_data_args = len(data_args)
        
        if isinstance(data, pd.DataFrame):
            if len(data.columns) == 1:
                data = data.iloc[:, 0]  # Convert single-column DataFrame to Series
            else:
                data = data.to_dict(orient="records")

        is_list_like = isinstance(data, (list, np.ndarray, pd.Series))
        is_list_of_dicts = is_list_like and all(isinstance(item, dict) for item in data)
    
        std_data = None
        if n_expected_data_args == 1:
            arg_name = self.data_args[0]
            if isinstance(data, str):
                std_data = [{arg_name: data}]
            elif is_list_like and is_list_of_dicts:
                std_data = data
            else:
                std_data = [{arg_name: item} for item in data]
        elif isinstance(data, dict):
            std_data = [data]
        elif is_list_like:
            if is_list_of_dicts:
                std_data = data
            else:
                raise ValueError(f"For multiple data args, list-like input must contain only dictionaries")
        
        if std_data is None:
            raise ValueError(f"Unsupported data type or structure for {n_expected_data_args} data args: {type(data)}")
        
        return [self.validate_datum(item, data_args) for item in std_data]

    @staticmethod
    def validate_datum(item, data_args):
        if not isinstance(item, dict):
            raise ValueError(f"Expected dict, got {type(item)}")
        if set(item.keys()) != set(data_args):
            raise ValueError(f"Mismatch in data arguments. Expected: {data_args}, Received: {list(item.keys())}")
        for key, value in item.items():
            if is_nan_or_none(value):
                raise ValueError(f"None or NaN value found for key '{key}'")
        return item

    @classmethod
    def from_samples(cls, 
                    data: pd.DataFrame,
                    n_samples: int,
                    sample_size: Union[int, List[int]],
                    random_state: Optional[int] = None,
                    labels: Optional[Dict[str, str]] = None,
                    separator: str = "-----") -> 'Dataset':
        """
        Create a Dataset instance from samples of a pandas DataFrame.

        This method generates multiple samples from the input DataFrame, where each sample
        is a combination of randomly selected rows. The resulting Dataset contains one
        observation per sample, where a "sample" is the unit of analysis to be passed to the
        LLM at inference time, with all columns from the original DataFrame combined into
        a single string representation.

        Args:
            data (pd.DataFrame): The input DataFrame to sample from.
            n_samples (int): The number of samples to generate.
            sample_size (Union[int, List[int]]): The number of rows to include in each sample.
                Can be a single integer for uniform sample sizes, or a list of integers for
                variable sample sizes. When a list is provided sample sizes are drawn randomly.
            random_state (Optional[int], default=None): Seed for the random number generator.
                If provided, ensures reproducibility of samples. Each sample uses a different
                seed derived from this base value.
            labels (Optional[Dict[str, str]], default=None): A dictionary to rename columns
                in the input DataFrame (i.e., to label variables differently for when they get
                passed to the LLM). Keys are original column names, values are new names that
                the LLM will see.
            separator (str, default="-----"): The string used to separate individual rows
                within a sample.

        Returns:
            Dataset: A new Dataset instance where each observation is a sample containing
            a sample of rows from the input DataFrame. The number of rows in each sample may
            vary if a list of sample sizes is provided.

        Raises:
            ValueError: If the input is not a pandas DataFrame, or if sample_size is neither a
            positive integer nor a list of positive integers.

        Example:
            >>> df = pd.DataFrame({
            ...     'id': range(1, 101),
            ...     'response': [f"Response {i}" for i in range(1, 101)],
            ...     'sentiment': ['positive', 'negative', 'neutral'] * 33 + ['positive']
            ... })
            >>> dataset = Dataset.from_samples(
            ...     data=df,
            ...     n_samples=5,
            ...     sample_size=3,
            ...     random_state=42,
            ...     labels={'id': 'ID', 'response': 'Response', 'sentiment': 'Sentiment'}
            ... )
            >>> print(dataset[0]['sample'])
            ID: 52
            Response: Response 52
            Sentiment: positive
            -----
            ID: 93
            Response: Response 93
            Sentiment: neutral
            -----
            ID: 15
            Response: Response 15
            Sentiment: neutral
        """
        
        # Data validation
        if isinstance(data, pd.DataFrame):
            if labels:
                data = data.rename(columns=labels)
        else:
            raise ValueError("Input data must be a pandas DataFrame")

        # Sample size validation
        if isinstance(sample_size, int):
            sample_size = [sample_size]
        elif not isinstance(sample_size, list) or not all(isinstance(s, int) and s > 0 for s in sample_size):
            raise ValueError("sample_size must be a positive integer or a list of positive integers")
        
        # Create an array of sampled sample sizes
        if random_state is not None:
            np.random.seed(random_state)
        sampled_sizes = np.random.choice(sample_size, size=n_samples)

        # Generate samples
        samples = []
        for n in range(n_samples):
            if random_state is not None:
                r = random_state + n
            else:
                r = None
            
            size = sampled_sizes[n]
            sample = data.sample(n=size, replace=True, random_state=r)
            sample_content = cls._combine_columns(sample, separator)
            sample_str = SampleString(
                content=sample_content,
                sample_size=size,
                dataset_size=len(data),
                indices=sample.index.tolist(),
                random_state=r
            )
            samples.append({"sample": sample_str})

        # Create and return a new Dataset instance
        return cls(samples, data_args=["sample"])

    @classmethod
    def from_cluster_samples(cls,
                            data: pd.DataFrame,
                            n_per_combination: int = 3,
                            sample_size: int = 5,
                            min_cluster_size: int = 15,
                            embedding_client: Optional['EmbeddingClient'] = None,
                            random_state: Optional[int] = None,
                            labels: Optional[Dict[str, str]] = None,
                            separator: str = "-----") -> 'Dataset':
        """Generate samples by clustering data and selecting from clusters in different ways.
        
        This method clusters your data based on semantic similarity, then creates samples
        by selecting observations from single clusters, pairs of clusters, and mixed
        cluster/random combinations. The goal is to create comparison sets that help 
        identify what characterizes and/or distinguishes different subsets in your data.
        
        Process:
        1. Embeds all text data to capture semantic meaning
        2. Clusters embeddings to identify thematically similar groups
        3. Creates four types of samples from these clusters:
           - Individual: All observations from one cluster
           - Pairs: Observations from two different clusters
           - Mixed: One cluster combined with random observations
           - Random: Pure random selection as baseline
        
        This systematic sampling is useful when you need to understand what differentiates
        groups in your data, as viewing similar items together makes their shared features
        apparent, while contrasting different groups highlights their distinctions.
        
        Args:
            data: DataFrame to sample from (must contain text data)
            n_per_combination: Number of samples per combination type (default=3)
            sample_size: Observations per sample (default=5)
            min_cluster_size: Minimum observations to form a cluster (default=15)
            embedding_client: Optional embedding client (auto-created if None)
            random_state: Random seed for reproducibility
            labels: Optional column name mapping for display
            separator: String to separate observations in output (default="-----")
            
        Returns:
            Dataset of strategically constructed samples for comparative analysis.
            Each sample contains `sample_size` observations from specified cluster combinations.
            
        Raises:
            ValueError: If sample_size <= 0 or n_per_combination <= 0
        """
        # Parameter validation
        if sample_size <= 0:
            raise ValueError("sample_size must be positive")
        if n_per_combination <= 0:
            raise ValueError("n_per_combination must be positive")
        # Allow min_cluster_size < sample_size in some cases (will use replacement sampling)
        from quallm.utils.clustering_utils import get_cluster_assignments
        from quallm.embedding_client import EmbeddingClient
        
        if embedding_client is None:
            embedding_client = EmbeddingClient()
        
        # Handle edge case: dataset too small to cluster
        if len(data) < 5:
            # Create a simple random sample instead
            sample_data = data.sample(n=min(sample_size, len(data)), replace=True, random_state=random_state)
            sample_string = _create_cluster_sample_string(
                sample_data, ['small_dataset'], 'random', len(data), labels, separator
            )
            return cls([{"sample": sample_string}], data_args=["sample"])
        
        # Get cluster assignments
        cluster_labels, metadata = get_cluster_assignments(
            data.iloc[:, 0], embedding_client, min_cluster_size=min_cluster_size,
            random_state=random_state
        )
        
        # Create DataFrame with cluster assignments
        clustered_data = data.copy()
        clustered_data['_cluster'] = cluster_labels
        
        # Handle edge case: all observations assigned to outliers
        cluster_sizes = clustered_data['_cluster'].value_counts()
        valid_clusters = cluster_sizes[cluster_sizes >= min_cluster_size].index.tolist()
        
        if not valid_clusters or (len(valid_clusters) == 1 and -1 in valid_clusters):
            # All outliers or no valid clusters - treat as single cluster
            sample_data = data.sample(n=min(sample_size, len(data)), replace=True, random_state=random_state)
            sample_string = _create_cluster_sample_string(
                sample_data, ['all_outliers'], 'random', len(data), labels, separator
            )
            return cls([{"sample": sample_string}], data_args=["sample"])
        
        # Generate samples with systematic coverage of cluster combinations
        samples = cls._generate_cluster_samples(
            clustered_data, valid_clusters, n_per_combination, 
            sample_size, len(data), random_state, labels, separator
        )
        
        return cls(samples, data_args=["sample"])

    @classmethod
    def _generate_cluster_samples(cls,
                                 clustered_data: pd.DataFrame,
                                 valid_clusters: List[int],
                                 n_per_combination: int,
                                 sample_size: int,
                                 total_dataset_size: int,
                                 random_state: Optional[int],
                                 labels: Optional[Dict[str, str]],
                                 separator: str) -> List[Dict[str, ClusterSampleString]]:
        """Generate samples with systematic coverage of cluster combinations.
        
        Creates four types of samples:
        - Individual cluster samples: Observations from a single cluster
        - Cluster pair samples: Observations from two different clusters
        - Cluster-random combinations: One cluster mixed with random observations
        - Pure random samples: Random observations as baseline comparison
        
        Each combination type gets n_per_combination samples. When a cluster has fewer
        observations than sample_size, sampling with replacement is used.
        """
        import itertools
        
        samples = []
        rng = np.random.RandomState(random_state)
        
        # Individual cluster samples 
        for cluster_id in valid_clusters:
            for _ in range(n_per_combination):
                cluster_data = clustered_data[clustered_data['_cluster'] == cluster_id]
                # Handle clusters smaller than sample_size with replacement
                replace_needed = len(cluster_data) < sample_size
                sample_data = cluster_data.sample(n=sample_size, replace=replace_needed, random_state=rng.randint(10000))
                # Randomize order of observations within sample
                sample_data = sample_data.sample(frac=1.0, random_state=rng.randint(10000))
                sample_string = _create_cluster_sample_string(
                    sample_data, [cluster_id], 'individual', total_dataset_size, labels, separator
                )
                samples.append({"sample": sample_string})
        
        # Cluster pair samples
        cluster_pairs = list(itertools.combinations(valid_clusters, 2))
        for pair in cluster_pairs:
            for _ in range(n_per_combination):
                combined_data = []
                for cluster_id in pair:
                    cluster_data = clustered_data[clustered_data['_cluster'] == cluster_id]
                    # Sample half the sample_size from each cluster (with balancing)
                    cluster_sample_size = sample_size // 2
                    if cluster_id == pair[1] and sample_size % 2 == 1:  # Give remainder to second cluster
                        cluster_sample_size += 1
                    
                    replace_needed = len(cluster_data) < cluster_sample_size
                    cluster_sample = cluster_data.sample(n=cluster_sample_size, replace=replace_needed, random_state=rng.randint(10000))
                    combined_data.append(cluster_sample)
                
                # Combine and randomize order
                sample_data = pd.concat(combined_data, ignore_index=False)
                sample_data = sample_data.sample(frac=1.0, random_state=rng.randint(10000))
                sample_string = _create_cluster_sample_string(
                    sample_data, list(pair), 'pair', total_dataset_size, labels, separator
                )
                samples.append({"sample": sample_string})
        
        # Cluster-random combinations (one cluster + random observations)
        for cluster_id in valid_clusters:
            for _ in range(n_per_combination):
                # Half from specific cluster, half random from entire dataset
                cluster_data = clustered_data[clustered_data['_cluster'] == cluster_id]
                cluster_sample_size = sample_size // 2
                
                replace_needed = len(cluster_data) < cluster_sample_size
                cluster_sample = cluster_data.sample(n=cluster_sample_size, replace=replace_needed, random_state=rng.randint(10000))
                
                # Random sample from entire dataset (excluding cluster column for sampling)
                random_sample_size = sample_size - cluster_sample_size
                random_sample = clustered_data.sample(n=random_sample_size, replace=True, random_state=rng.randint(10000))
                
                # Combine and randomize order
                sample_data = pd.concat([cluster_sample, random_sample], ignore_index=False)
                sample_data = sample_data.sample(frac=1.0, random_state=rng.randint(10000))
                sample_string = _create_cluster_sample_string(
                    sample_data, [cluster_id, 'random'], 'cluster-random', total_dataset_size, labels, separator
                )
                samples.append({"sample": sample_string})
        
        # Pure random samples (baseline)
        for _ in range(n_per_combination):
            sample_data = clustered_data.sample(n=sample_size, replace=True, random_state=rng.randint(10000))
            sample_string = _create_cluster_sample_string(
                sample_data, ['random'], 'random', total_dataset_size, labels, separator
            )
            samples.append({"sample": sample_string})
        
        return samples

    @staticmethod
    def _combine_columns(sample: pd.DataFrame, separator: str) -> str:
        combined = []
        for _, row in sample.iterrows():
            row_str = "\n".join(f"{col}: {val}" for col, val in row.items() if pd.notna(val))
            combined.append(row_str)
        return f"\n{separator}\n".join(combined)
    
    def get_data_summary_string(self, config: Optional['FeedbackConfig'] = None) -> str:
        """
        Extract schema and compute descriptive statistics.
        
        Returns a formatted string summary of the dataset structure with statistics
        for each field based on its data type.
        
        Args:
            config: Optional FeedbackConfig for controlling display settings
        
        Returns:
            Formatted string with data schema and statistics
        
        Example:
            >>> dataset.get_data_summary()
            "Data schema (20 examples):\\nFields:\\n  - text: type=str, missing=0, ..."
        """
        from quallm.utils.dataframe_utils import (
            infer_column_type, compute_string_stats,
            compute_list_stats, safe_describe, format_float
        )
        from quallm.feedback_config import FeedbackConfig
        
        if config is None:
            config = FeedbackConfig()
        
        # Convert to DataFrame for analysis
        df = pd.DataFrame(self)
        
        schema_lines = [f"Data schema ({len(df)} examples):", "Fields:"]
        
        for col in df.columns:
            missing = df[col].isna().sum()
            type_name, type_class = infer_column_type(df[col])
            
            if type_name == 'unknown':
                stats = f"type=unknown, missing={missing}"
            elif type_class == str:
                # String statistics
                str_stats = compute_string_stats(df[col])
                stats = f"type=str, missing={missing}, empty={str_stats['empty_count']}, "
                stats += f"length_stats={{min={str_stats['min_length']}, max={str_stats['max_length']}, "
                stats += f"mean={format_float(str_stats['mean_length'], 1)}, median={format_float(str_stats['median_length'], 1)}}}"
            elif type_class == list:
                # List statistics
                list_stats = compute_list_stats(df[col])
                stats = f"type=list, missing={missing}, "
                stats += f"length_stats={{min={list_stats['min_length']}, max={list_stats['max_length']}, "
                stats += f"mean={format_float(list_stats['mean_length'], 1)}}}"
            elif df[col].dtype in ['int64', 'float64']:
                # Numeric statistics
                num_stats = safe_describe(df[col])
                stats = f"type={df[col].dtype}, missing={missing}, "
                stats += f"stats={{min={format_float(num_stats['min'])}, max={format_float(num_stats['max'])}, "
                stats += f"mean={format_float(num_stats['mean'])}, std={format_float(num_stats['std'])}}}"
            else:
                # Other types
                stats = f"type={type_name}, missing={missing}, unique={df[col].nunique()}"
            
            schema_lines.append(f"  - {col}: {stats}")
        
        return "\n".join(schema_lines)
    
    def format_data_item(self, index: int, config: Optional['FeedbackConfig'] = None) -> str:
        """
        Format a single data item for display with intelligent truncation.
        
        Args:
            index: Index of the data item to format
            config: Optional FeedbackConfig for controlling display settings
        
        Returns:
            Formatted string representation of the data item
        
        Example:
            >>> dataset.format_data_item(0)
            "text: 'This is a sample text...' (150 chars)\\nrating: 5"
        """
        from quallm.utils.dataframe_utils import truncate_string, format_list_preview
        from quallm.feedback_config import FeedbackConfig
        
        if config is None:
            config = FeedbackConfig()
        
        if index >= len(self):
            raise IndexError(f"Index {index} out of range for dataset of length {len(self)}")
        
        item = self[index]
        formatted_lines = []
        
        for key, value in item.items():
            if pd.isna(value):
                display = "None"
            elif isinstance(value, str):
                if len(value) > config.max_text_length:
                    display = truncate_string(value, config.max_text_length)
                else:
                    display = repr(value)
            elif isinstance(value, list):
                display = format_list_preview(value, max_items=3)
            else:
                display = repr(value)
            
            formatted_lines.append(f"  {key}: {display}")
        
        return "\n".join(formatted_lines)
    
    def format_observations(self, predictions: Optional['Prediction'] = None,
                           config: Optional['FeedbackConfig'] = None,
                           processed_indices: Optional[List[int]] = None) -> str:
        """
        Format observations for display in nested XML format, optionally with outputs.
        
        Args:
            predictions: Optional Prediction object with task execution results
            config: Configuration for display settings
            processed_indices: Indices of data items that were processed (for Mode 2).
                              If None (Mode 1), will sample from all available data
        
        Returns:
            XML-formatted string with observations (inputs and optionally outputs)
        
        Example output:
            Showing 3 of 20 processed examples:
            <observation>
              <input>
                text: "This product exceeded my expectations..." (245 chars)
                rating: 5
              </input>
              <output>
                sentiment: 'positive'
                confidence: 9
              </output>
            </observation>
        """
        from quallm.feedback_config import FeedbackConfig
        
        if config is None:
            config = FeedbackConfig()
        
        # Determine which indices to display
        if processed_indices is not None:
            # Mode 2: We have processed specific indices
            available_indices = processed_indices
            header = f"Showing {min(config.max_examples_to_show, len(available_indices))} of {len(available_indices)} processed examples:"
        else:
            # Mode 1: No processing done, sample from all data
            available_indices = list(range(len(self)))
            header = f"Showing {min(config.max_examples_to_show, len(self))} examples:"
        
        # Sample indices to display
        import random
        n_to_show = min(config.max_examples_to_show, len(available_indices))
        if n_to_show == 0:
            return "No examples to show"
        
        display_indices = random.sample(available_indices, n_to_show)
        display_indices.sort()  # Keep in order
        
        observations = [header]
        
        for idx in display_indices:
            obs_lines = ["<observation>", "  <input>"]
            
            # Format input
            input_formatted = self.format_data_item(idx, config)
            for line in input_formatted.split('\n'):
                obs_lines.append("  " + line)
            obs_lines.append("  </input>")
            
            # Format output if predictions available
            if predictions is not None:
                # Find the position in predictions array
                if processed_indices is not None:
                    pred_idx = processed_indices.index(idx)
                else:
                    pred_idx = idx
                
                obs_lines.append("  <output>")
                output_formatted = predictions.format_output_item(pred_idx, rater_idx=0)
                for line in output_formatted.split('\n'):
                    obs_lines.append("  " + line)
                obs_lines.append("  </output>")
            
            obs_lines.append("</observation>")
            observations.append("\n".join(obs_lines))
        
        return "\n".join(observations)


def _create_cluster_sample_string(sample_data: pd.DataFrame,
                                 source_clusters: List[Union[int, str]], 
                                 combination_type: str,
                                 total_dataset_size: int,
                                 labels: Optional[Dict[str, str]],
                                 separator: str) -> ClusterSampleString:
    """Helper function to create ClusterSampleString instances."""
    # Format sample content using established separator pattern
    if labels:
        display_data = sample_data.rename(columns=labels)
    else:
        display_data = sample_data.drop('_cluster', axis=1, errors='ignore')
    
    content = Dataset._combine_columns(display_data, separator)
    
    cluster_stats = {
        'n_clusters_used': len([c for c in source_clusters if c != 'random']),
        'combination_type': combination_type,
        'cluster_sizes': {}  # Could be populated with actual cluster sizes
    }
    
    return ClusterSampleString(
        content=content,
        sample_size=len(sample_data),
        dataset_size=total_dataset_size,
        indices=sample_data.index.tolist(),
        random_state=None,  # Could track if needed
        source_clusters=source_clusters,
        combination_type=combination_type,
        cluster_stats=cluster_stats
    )